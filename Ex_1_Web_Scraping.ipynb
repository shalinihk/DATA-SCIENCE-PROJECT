{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stuODxvnvckR",
    "outputId": "0dcd272c-68dc-452b-8350-d3459ca05bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Web Scraping Mini-Project ---\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-1.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-2.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-3.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-4.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-5.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-6.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-7.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-8.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-9.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-10.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-11.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-12.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-13.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-14.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-15.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-16.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-17.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-18.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-19.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-20.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-21.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-22.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-23.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-24.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-25.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-26.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-27.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-28.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-29.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-30.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-31.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-32.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-33.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-34.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-35.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-36.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-37.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-38.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-39.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-40.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-41.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-42.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-43.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-44.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-45.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-46.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-47.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-48.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-49.html\n",
      "Scraping data from: http://books.toscrape.com/catalogue/page-50.html\n",
      "\n",
      "Successfully scraped a total of 1000 books from 50 pages.\n",
      "Data successfully saved to books_data.csv\n",
      "\n",
      "--- Process Complete ---\n"
     ]
    }
   ],
   "source": [
    "# To run this script, you need to install the following libraries:\n",
    "# pip install requests beautifulsoup4\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_books_from_website():\n",
    "    \"\"\"\n",
    "    Scrapes book title, price, and star rating from a fictional books website.\n",
    "    This demonstrates collecting structured data from an unstructured HTML page.\n",
    "    The script now scrapes data from all pages of the website.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Web Scraping Mini-Project ---\")\n",
    "\n",
    "    # The URL of the website we want to scrape. This is a practice site.\n",
    "    base_url = \"http://books.toscrape.com/catalogue/\"\n",
    "    next_page_url = \"page-1.html\" # Start with the first page\n",
    "\n",
    "    all_scraped_books = []\n",
    "    page_count = 0\n",
    "\n",
    "    # The loop continues as long as there is a \"Next\" page link to follow.\n",
    "    while next_page_url:\n",
    "        page_count += 1\n",
    "        current_url = requests.compat.urljoin(base_url, next_page_url)\n",
    "        print(f\"Scraping data from: {current_url}\")\n",
    "\n",
    "        try:\n",
    "            # Send a GET request to the URL to get the HTML content.\n",
    "            response = requests.get(current_url)\n",
    "\n",
    "            # Raise an exception for bad status codes (4xx or 5xx).\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Use BeautifulSoup to parse the HTML content of the page.\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all HTML elements that contain book information.\n",
    "            # Each book is contained within an <article> tag with the class 'product_pod'.\n",
    "            book_articles = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "            for book in book_articles:\n",
    "                # Extract the book title from the <h3> tag's <a> tag's title attribute.\n",
    "                title = book.h3.a['title']\n",
    "\n",
    "                # Extract the price from the <p> tag with the class 'price_color'.\n",
    "                price_str = book.find('p', class_='price_color').text\n",
    "                # Clean the price string to remove the currency symbol and convert to float.\n",
    "                price = float(price_str.replace('Â£', '').strip())\n",
    "\n",
    "                # Extract the star rating. The class name itself contains the rating.\n",
    "                # Example: <p class=\"star-rating Three\">\n",
    "                rating_class = book.find('p', class_='star-rating')['class'][1]\n",
    "                rating = rating_class\n",
    "\n",
    "                all_scraped_books.append({\n",
    "                    'title': title,\n",
    "                    'price': price,\n",
    "                    'rating': rating\n",
    "                })\n",
    "\n",
    "            # Find the link for the next page.\n",
    "            # The \"Next\" link is inside a li tag with the class \"next\".\n",
    "            next_button = soup.find('li', class_='next')\n",
    "            if next_button:\n",
    "                next_page_url = next_button.a['href']\n",
    "            else:\n",
    "                # If there's no \"Next\" button, we have reached the last page.\n",
    "                next_page_url = None\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data from website: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing HTML: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(f\"\\nSuccessfully scraped a total of {len(all_scraped_books)} books from {page_count} pages.\")\n",
    "\n",
    "    return all_scraped_books\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"\n",
    "    Saves a list of dictionaries to a CSV file.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(f\"No data to save to {filename}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Extract column headers from the keys of the first dictionary.\n",
    "    keys = data[0].keys()\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "            # Create a CSV writer object.\n",
    "            dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "\n",
    "            # Write the header row.\n",
    "            dict_writer.writeheader()\n",
    "\n",
    "            # Write all the data rows.\n",
    "            dict_writer.writerows(data)\n",
    "\n",
    "        print(f\"Data successfully saved to {filename}\")\n",
    "\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving data to CSV file: {e}\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    books_data = scrape_books_from_website()\n",
    "\n",
    "    if books_data:\n",
    "        save_to_csv(books_data, 'books_data.csv')\n",
    "\n",
    "    print(\"\\n--- Process Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vqojLCLdFV-u"
   },
   "outputs": [],
   "source": [
    "## As part of this mini-project, you are required to collect real estate data from online property listing websites such as 99acres, NoBroker, and MagicBricks.\n",
    "## Use a combination of API access (if available) and web scraping techniques to extract relevant property features including location, total square feet, number of bedrooms (BHK),\n",
    "## number of bathrooms, number of balconies, area type (such as super built-up area, carpet area, or plot), and the price of the property (in INR or Lakhs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
